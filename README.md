# ğŸš€ NASA APOD ETL Pipeline

> **Extract, Transform, Load pipeline for NASA's Astronomy Picture of the Day using Apache Airflow and PostgreSQL**

## ğŸ“‹ Overview

This project implements an automated ETL pipeline that:
- ğŸŒŒ Extracts daily astronomy pictures from NASA's APOD API
- ğŸ”„ Transforms the data for storage optimization
- ğŸ’¾ Loads processed data into PostgreSQL database
- ğŸ“Š Enables analytics and reporting on astronomical content

## ğŸ—ï¸ Project Structure

```
ETL_pipeline/
â”œâ”€â”€ ğŸ“ dags/              # Airflow DAG definitions
â”‚   â””â”€â”€ ETL.py           # Main ETL workflow
â”œâ”€â”€ ğŸ“ sql/              # Database schema scripts
â”œâ”€â”€ ğŸ“ config/           # Configuration files
â”œâ”€â”€ ğŸ“ scripts/          # Utility scripts
â”œâ”€â”€ ğŸ“„ requirements.txt  # Python dependencies
â”œâ”€â”€ ğŸ³ Dockerfile        # Container configuration
â””â”€â”€ ğŸ³ docker-compose.yml # Multi-container setup
```

## ğŸ› ï¸ Technology Stack

- **ğŸŒŠ Apache Airflow**: Workflow orchestration
- **ğŸš€ Astronomer CLI**: Pipeline deployment and management
- **â˜ï¸ AWS RDS PostgreSQL**: Cloud database storage
- **ğŸ Python**: Data processing
- **ğŸ³ Docker**: Containerization
- **ğŸ”— NASA API**: Data source
- **ğŸ‘ï¸ DBeaver**: Database visualization and management

## âš™ï¸ Setup Instructions

### 1. ğŸš€ Start the Environment with Astronomer CLI
```bash
# Initialize Astronomer project
astro dev init

# Start local Airflow environment
astro dev start
```

### 2. â˜ï¸ AWS RDS Setup
- Create PostgreSQL instance on AWS RDS
- Configure security groups for access
- Note down connection details (endpoint, port, database name)

### 3. ğŸŒ Access Services
- **Airflow UI**: http://localhost:8080
- **AWS RDS PostgreSQL**: Your RDS endpoint:5432
- **DBeaver**: Connect using RDS credentials

### 4. ğŸ”§ Configure Database Connection
Create a new connection in Airflow UI:
- **Connection ID**: `postgres_connection`
- **Connection Type**: Postgres
- **Host**: Your AWS RDS endpoint
- **Schema**: Your database name
- **Port**: `5432`
- **Login**: Your RDS username
- **Password**: Your RDS password

## ğŸ¯ Pipeline Features

### ğŸ“¥ Data Extraction
- ğŸŒŸ NASA APOD API integration
- ğŸ”„ Daily automated data retrieval
- ğŸ›¡ï¸ Error handling and retry logic

### ğŸ”„ Data Transformation
- ğŸ“ Data cleaning and validation
- ğŸ¯ Field selection and formatting
- ğŸ“Š Data quality checks

### ğŸ“¤ Data Loading
- âš¡ Efficient bulk loading
- ğŸ”’ Transaction safety
- ğŸ“ˆ Performance optimization

### ğŸ“Š Monitoring & Logging
- ğŸ“‹ Pipeline status tracking
- ğŸš¨ Error alerting
- ğŸ“ Comprehensive logging

## ğŸš€ Deployment

### ğŸŒŸ Local Development
```bash
# Start local development environment
astro dev start

# View logs
astro dev logs

# Stop environment
astro dev stop
```

### â˜ï¸ Production Deployment with Astronomer
```bash
# Deploy to Astronomer Cloud
astro deploy

# Check deployment status
astro deployment list
```

### ğŸ‘ï¸ Data Visualization with DBeaver
1. **Install DBeaver**: Download from https://dbeaver.io/
2. **Connect to AWS RDS**:
   - Host: Your RDS endpoint
   - Port: 5432
   - Database: Your database name
   - Username/Password: Your RDS credentials
3. **View Data**: Browse `nasa_apod` table and run queries

## ğŸš€ Usage

1. **ğŸ“‚ Setup Data Sources**: Configure NASA API access
2. **âš™ï¸ Configure Connections**: Set up database connections in Airflow
3. **â–¶ï¸ Run Pipeline**: Execute the `nasa_apod_postgres` DAG
4. **ğŸ“Š Monitor Results**: Check Airflow UI and PostgreSQL for data

## ğŸ“Š Database Schema

The pipeline creates a `nasa_apod` table with:
- ğŸ†” `id`: Primary key
- ğŸ“ `title`: Picture title
- ğŸ“– `explanation`: Detailed description
- ğŸ”— `url`: Image URL
- ğŸ“… `date`: Publication date
- ğŸ¬ `media_type`: Content type (image/video)

## ğŸ”§ Troubleshooting

### Common Issues:
- ğŸ˜ **AWS RDS Connection**: Ensure security groups allow inbound connections
- ğŸš€ **Astronomer CLI**: Verify CLI installation and authentication
- ğŸ”‘ **API Access**: Verify NASA API key configuration
- ğŸ“ **File Permissions**: Check Docker volume permissions
- ğŸŒ **Network**: Confirm VPC and subnet configurations
- ğŸ‘ï¸ **DBeaver Connection**: Verify RDS endpoint and credentials

### ğŸ“ Logs Location:
- Airflow logs: Available in Airflow UI or `astro dev logs`
- Astronomer deployment logs: `astro deployment logs`
- Container logs: `docker-compose logs`

## ğŸ¤ Contributing

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch
3. ğŸ’» Make your changes
4. ğŸ§ª Test thoroughly
5. ğŸ“¤ Submit a pull request

## ğŸ“„ License

This project is open source and available under the MIT License.

---

**ğŸŒŸ Happy Data Engineering! ğŸŒŸ**